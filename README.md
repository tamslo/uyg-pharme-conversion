# UYG to PharMe Data Conversion

Dockerfile and instructions to convert (my) genetic data from the (2020) UYG
course :bulb: to a format that can be used by PharMe. :dna::pill:

## Prerequisites

* Your genetic data in 23andMe format or in PLINK format
* Docker installed
* Potentially rename your data to match commands (or rename the paths in the
  commands below): `data/data.txt` (23andMe format) or in
    `data/data.bim`, `data/data.bed`, `data/data.fam` (PLINK format)
* Build Docker container `docker build -t uyg-to-pharme .`
* Potentially download reference files as needed, using
   `docker run --rm -v ./data:/data -w /data uyg-to-pharme bash scripts/download_reference_data.sh`
   (you may not need all data or any data, if the PharmCAT preprocessing works
   for you, you can comment out what you will not need)

## How to use

1. Convert your data to VCF
   * If your data is in PLINK format, first convert it to 23andMe format:

     ```bash
     docker run --rm -v ./data:/data -w /data uyg-to-pharme \
       plink --bed data.bed --bim data.bim --fam data.fam \
         --recode 23 --out data
     ```

   * To convert the 23andMe format, do:

     ```bash
     docker run --rm -v ./data:/data -w /data uyg-to-pharme \
       bcftools convert  -c ID,CHROM,POS,AA --tsv2vcf data.txt \
         -f /data/references/genomes/GRCh37.23andMe.fa -s Sample -Oz -o data.vcf
     ```

     (for more information refer to this
     [BCFtools tutorial](https://samtools.github.io/bcftools/howtos/convert.html))

2. Preprocess VCF file ([Docs](https://pharmcat.org/using/VCF-Preprocessor/));
   if you run into any problems, such as a lot of missing variants, please
   refer to the [Manual Preprocessing](#manual-preprocessing) section

     ```bash
     docker run --rm -v ./data:/data -w /data pgkb/pharmcat \
       pharmcat_vcf_preprocessor -v -vcf data.vcf
     ```

3. Optionally, impute your data (see [Imputation](#imputation))
4. Run PharmCAT

     ```bash
     docker run --rm -v ./data:/data -w /data pgkb/pharmcat \
       pharmcat_pipeline data.[preprocessed|imputed].vcf[.gz]

     ```

## Reference Genome Notes

Different reference genomes are available from different sources, that may
differ in their notations, e.g.:

* The Ensembl hg19 reference genome:
  [Ensembl GRCh37](https://ftp.ensembl.org/pub/grch37/current/fasta/homo_sapiens/dna/Homo_sapiens.GRCh37.dna.primary_assembly.fa.gz)
* The NCBI hg19 reference genome:
  [NCBI GRCh37](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.13/)
* The NCBI hg38 reference genome (what PharMe works with):
  [GRCh38.p13](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.39/)

The NCBI reference genomes are loaded and preprocessed in the
`download_reference_data.sh` script.

## Manual Preprocessing

Manual preprocessing steps to fix and/or clarify problems. Also see
[PharmCAT Docs](https://pharmcat.org/using/VCF-Requirements).

### Liftover

Manual liftover to GRCh38.p13 (the reference genome used by PharmCAT) using
GATK/Picard.

You may want to try other liftOver tools, please make sure that not only the
coordinates but also the genotypes are updated.

This is taking a while, on an M3 MacBook Air with 16GB RAM it took about 5h and
30min for me. You can maybe play around with the memory settings `-Xmx12G` and
parameters such as `--MAX_RECORDS_IN_RAM`.
From the documentation: *250k reads per GB given to the -Xmx parameter* (see
[FAQ](https://broadinstitute.github.io/picard/faq.html)).
  
```bash
docker run --rm -v ./data:/data broadinstitute/gatk:4.1.3.0 ./gatk \
  CreateSequenceDictionary -R /data/references/genomes/GRCh38.p13.23andMe.fa
docker run --rm -v ./data:/data broadinstitute/gatk:4.1.3.0 ./gatk LiftoverVcf \
  --java-options  "-Xmx12G" \
  --MAX_RECORDS_IN_RAM 3000000 \
  --TMP_DIR /data/liftover-temp \
  -I /data/data.vcf \
  -O /data/data.hg38.vcf \
  --CHAIN /data/references/GRCh37_to_GRCh38.chain.gz \
  --REJECT /data/liftover_rejected_variants.vcf \
  -R /data/references/genomes/GRCh38.p13.23andMe.fa
```

### Imputation

... using [Beagle](https://faculty.washington.edu/browning/beagle/beagle.html)
(also see the
[Documentation](https://faculty.washington.edu/browning/beagle/beagle_5.5_17Dec24.pdf)).

‚ÑπÔ∏è The imputation script already takes care of all the other preprocessing
steps on the single chromosome files (otherwise some commands may fail on the
large merged file).

```bash
docker run --rm -v ./data:/data -w /data uyg-to-pharme \
  bash scripts/impute.sh data.hg38.vcf data.imputed.vcf.gz
```

#### Workaround

‚ö†Ô∏è _Currently PharmCAT directly using the `data.imputed.vcf.gz` yields less_
_results than the original preprocessed file._

Run PharmCAT with your non-imputed data first and amend imputed variants that
are included in the `data.preprocessed.missing_pgx_var.vcf` file.

```bash
docker run -it --rm -v ./data:/data -w /data uyg-to-pharme
# First intersect the imputed data with positions that are interesting
bgzip data.preprocessed.missing_pgx_var.vcf
tabix -p vcf data.preprocessed.missing_pgx_var.vcf.gz
tabix -p vcf data.imputed.vcf.gz
bcftools isec -p isec_output -Oz data.imputed.vcf.gz data.preprocessed.missing_pgx_var.vcf.gz

# Merge imputed and missing variants into preprocessed data
bgzip -d data.preprocessed.vcf.bgz
bgzip data.preprocessed.vcf
tabix -p vcf data.preprocessed.vcf.gz
bcftools concat -a data.preprocessed.vcf.gz isec_output/0002.vcf.gz -o data.concat.preprocessed.imputed.vcf.gz
# TODO: getting bgzip error
tabix -p vcf data.concat.preprocessed.imputed.vcf.gz
bcftools sort data.concat.preprocessed.imputed.vcf.gz -Oz -o data.final.preprocessed.imputed.vcf.gz
```

#### Caveats

‚ö†Ô∏è _I had a problem for the Y chromosome reports, may be fixed if you actually_
_have a Y chromosome; however, changing the ploidy to diploid for all Y_
_variants for diploid in `apapt_y_ploidy.py` as part of the `impute.sh` script_
_for now._

‚ö†Ô∏è _The normalization tool has problems with merging description fields of_
_some variants with missing genotype calls with the same position, therefore a_
_script removes the INFO and FORMAT fields added in the imputation._

‚ö†Ô∏è _Known problem: the normalization may fail with_
_`Error at <chr>:<pos>: incorrect allele index 1`_

_in this case, please review and update the `imputed.chr<chr>.clean.vcf.gz`_
_file manually at `<pos>`, i.e., decompress, decide which variant to keep, and_
_compress updated (see_
_[Inspecting Intermediate Files](#inspecting-intermediate-files)), e.g.:_

```bash
docker run -it --rm -v ./data:/data -w /data uyg-to-pharme
currentChrom=<chr>
# Delete incomplete normalization file
rm imputation-temp/imputed.chr$currentChrom.normalized.vcf.gz
bgzip -d imputation-temp/imputed.chr$currentChrom.clean.vcf.gz
# Manually edit file at <pos> and potentially keep a record of your changes
bgzip imputation-temp/imputed.chr$currentChrom.clean.vcf
```

### Normalization

```bash
docker run --rm -v ./data:/data -w /data uyg-to-pharme \
  bash scripts/normalize.sh data.hg38.vcf data.normalized.vcf
```

### Sort by Position

```bash
docker run --rm -v ./data:/data -w /data uyg-to-pharme \
  bash scripts/sort.sh data.normalized.vcf data.sorted.vcf
```

### Chromosome Fix

To prefix the chromosome with `chr`, use

```bash
docker run --rm -v ./data:/data -w /data uyg-to-pharme \
  bash scripts/fix_chromosomes.sh data.sorted.vcf data.preprocessed.vcf
```

### Inspecting Intermediate Files

This may be helpful when you encounter preprocessing errors.

You can check the content of (intermediate) compressed processed VCFs (for
manual preprocessing or when running the PharmCAT preprocessing command with
the `-k` option) with:

`docker run --rm -v ./data:/data -w /data uyg-to-pharme bgzip -d <file.[b]gz>`

Compress files again without the `-d` option.

Sometimes it may be easier to connect with the container and use the shell
inside:

`docker run -it --rm -v ./data:/data -w /data uyg-to-pharme`

## Load Into PharMe

üöß _TODO: describe how to load into PharMe; e.g., without PharmCAT and master's_
_project or properly parse and upload to (local) lab server setup. But one_
_could also check the PharmCAT output, I guess someone who can make these_
_scripts work will manage with the "expert version"._ üòä
